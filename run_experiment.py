#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ChartGalaxyæ•°æ®é©±åŠ¨ä¿¡æ¯å›¾ç”Ÿæˆå®éªŒä¸»è„šæœ¬

æ•´åˆæ‰€æœ‰æ¨¡å—ï¼Œå®ç°å®Œæ•´çš„ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–å®éªŒæµç¨‹ï¼š
1. æ•°æ®é¢„å¤„ç†
2. å®éªŒçŸ©é˜µç”Ÿæˆ
3. æç¤ºè¯ç”Ÿæˆ
4. AIå›¾åƒç”Ÿæˆ
5. AIè‡ªåŠ¨è¯„ä¼°
6. å®éªŒæŠ¥å‘Šç”Ÿæˆ

ä½œè€…: ChartGalaxy Pipeline
æ—¥æœŸ: 2024
"""

import os
import json
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from data_preprocessor import DataPreprocessor
from experiment_matrix import ExperimentMatrix
from prompt_generator import PromptGenerator
from evaluation_framework import EvaluationFramework
from ai_image_generator import AIImageGenerator, create_generator_from_config
from ai_evaluator import AIEvaluator, create_evaluator_from_config

class ExperimentRunner:
    """å®éªŒè¿è¡Œå™¨ç±»"""
    
    def __init__(self, 
                 benchmark_data_dir: str = "benchmark_data",
                 output_dir: str = "experiment_output",
                 config_path: str = None):
        """
        åˆå§‹åŒ–å®éªŒè¿è¡Œå™¨
        
        Args:
            benchmark_data_dir: MatPlotBenchæ•°æ®ç›®å½•
            output_dir: å®éªŒè¾“å‡ºç›®å½•
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.benchmark_data_dir = Path(benchmark_data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # åŠ è½½é…ç½®
        self.config = self._load_config(config_path)
        
        # åˆ›å»ºå­ç›®å½•
        self.processed_data_dir = self.output_dir / "processed_data"
        self.prompts_dir = self.output_dir / "prompts"
        self.evaluations_dir = self.output_dir / "evaluations"
        self.reports_dir = self.output_dir / "reports"
        
        for dir_path in [self.processed_data_dir, self.prompts_dir, 
                        self.evaluations_dir, self.reports_dir]:
            dir_path.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.data_processor = None
        self.experiment_matrix = None
        self.prompt_generator = None
        self.evaluation_framework = None
        
        # å®éªŒçŠ¶æ€
        self.experiment_id = None
        self.start_time = None
        self.experiment_log = []
    
    def initialize_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰å®éªŒç»„ä»¶"""
        self.log("æ­£åœ¨åˆå§‹åŒ–å®éªŒç»„ä»¶...")
        
        try:
            # åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨ï¼ˆä¼ å…¥é…ç½®ï¼‰
            self.data_processor = DataPreprocessor(
                str(self.benchmark_data_dir),
                self.config
            )
            self.log("æ•°æ®å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–å®éªŒçŸ©é˜µ
            self.experiment_matrix = ExperimentMatrix()
            self.log("å®éªŒçŸ©é˜µåˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–æç¤ºè¯ç”Ÿæˆå™¨ï¼ˆä¼ å…¥é…ç½®ï¼‰
            self.prompt_generator = PromptGenerator(self.config)
            self.log("æç¤ºè¯ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–è¯„ä¼°æ¡†æ¶ï¼ˆä¼ å…¥é…ç½®ï¼‰
            self.evaluation_framework = EvaluationFramework(self.config)
            self.log("è¯„ä¼°æ¡†æ¶åˆå§‹åŒ–å®Œæˆ")
            
            self.log("æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            self.log(f"ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def log(self, message: str, level: str = "INFO"):
        """è®°å½•å®éªŒæ—¥å¿—"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] {level}: {message}"
        print(log_entry)
        self.experiment_log.append(log_entry)
    
    def start_experiment(self, experiment_name: str = None) -> str:
        """å¼€å§‹å®éªŒ"""
        self.start_time = datetime.now()
        # Format time string outside f-string to avoid backslash issue
        time_str = self.start_time.strftime('%Y%m%d_%H%M%S')
        self.experiment_id = experiment_name or f"exp_{time_str}"
        
        self.log(f"å¼€å§‹å®éªŒ: {self.experiment_id}")
        self.log(f"å®éªŒå¼€å§‹æ—¶é—´: {self.start_time}")
        
        return self.experiment_id
    
    def step1_process_data(self) -> bool:
        """æ­¥éª¤1: æ•°æ®é¢„å¤„ç†"""
        self.log("=" * 50)
        self.log("æ­¥éª¤1: æ•°æ®é¢„å¤„ç†")
        self.log("=" * 50)
        
        try:
            # ç”Ÿæˆå®éªŒæ•°æ®æ ·æœ¬
            self.log("æ­£åœ¨ç”Ÿæˆå®éªŒæ•°æ®æ ·æœ¬...")
            sample_files = self.data_processor.generate_experiment_samples()
            self.log(f"æˆåŠŸç”Ÿæˆ {len(sample_files)} ä¸ªæ•°æ®æ ·æœ¬")
            
            # åŠ è½½ç”Ÿæˆçš„æ ·æœ¬æ–‡ä»¶
            processed_samples = []
            for i, sample_file in enumerate(sample_files, 1):
                self.log(f"æ­£åœ¨åŠ è½½æ ·æœ¬æ–‡ä»¶ {i}/3: {sample_file}")
                
                try:
                    with open(sample_file, 'r', encoding='utf-8') as f:
                        sample_data = json.load(f)
                    processed_samples.append(sample_data)
                    self.log(f"æ ·æœ¬æ–‡ä»¶ {sample_file} åŠ è½½å®Œæˆ")
                    
                except Exception as e:
                    self.log(f"åŠ è½½æ ·æœ¬æ–‡ä»¶ {sample_file} æ—¶å‡ºé”™: {e}", "ERROR")
                    continue
            
            # ä¿å­˜å¤„ç†åçš„æ•°æ®
            processed_data_file = self.processed_data_dir / "processed_samples.json"
            with open(processed_data_file, 'w', encoding='utf-8') as f:
                json.dump(processed_samples, f, ensure_ascii=False, indent=2)
            
            self.log(f"å¤„ç†åçš„æ•°æ®å·²ä¿å­˜åˆ°: {processed_data_file}")
            self.log(f"æ•°æ®é¢„å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(processed_samples)} ä¸ªæ ·æœ¬")
            
            return len(processed_samples) > 0
            
        except Exception as e:
            self.log(f"æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}", "ERROR")
            return False
    
    def step2_generate_matrix(self) -> bool:
        """æ­¥éª¤2: ç”Ÿæˆå®éªŒçŸ©é˜µ"""
        self.log("=" * 50)
        self.log("æ­¥éª¤2: ç”Ÿæˆå®éªŒçŸ©é˜µ")
        self.log("=" * 50)
        
        try:
            # ç”Ÿæˆå®éªŒçŸ©é˜µ
            matrix_df = self.experiment_matrix.generate_matrix()
            self.log(f"ç”Ÿæˆå®éªŒçŸ©é˜µï¼Œå…± {len(matrix_df)} ä¸ªå®éªŒæ¡ä»¶")
            
            # ä¿å­˜å®éªŒçŸ©é˜µ
            matrix_file = self.output_dir / "experimental_matrix.csv"
            matrix_df.to_csv(matrix_file, index=False, encoding='utf-8-sig')
            self.log(f"å®éªŒçŸ©é˜µå·²ä¿å­˜åˆ°: {matrix_file}")
            
            # æ‰“å°çŸ©é˜µæ‘˜è¦
            self.log("å®éªŒçŸ©é˜µæ‘˜è¦:")
            self.log(f"  - æ•°æ®æ ·æœ¬: {matrix_df['Data Sample ID'].nunique()} ä¸ª")
            self.log(f"  - å›¾è¡¨ç±»å‹: {matrix_df['Chart Type'].nunique()} ç§")
            self.log(f"  - å¸ƒå±€æ¨¡æ¿: {matrix_df['Layout Template ID'].nunique()} ç§")
            self.log(f"  - æ€»å®éªŒæ•°: {len(matrix_df)} ä¸ª")
            
            return True
            
        except Exception as e:
            self.log(f"ç”Ÿæˆå®éªŒçŸ©é˜µå¤±è´¥: {e}", "ERROR")
            return False
    
    def step3_generate_prompts(self) -> bool:
        """æ­¥éª¤3: ç”Ÿæˆæç¤ºè¯"""
        self.log("=" * 50)
        self.log("æ­¥éª¤3: ç”Ÿæˆæç¤ºè¯")
        self.log("=" * 50)
        
        # æ£€æŸ¥æç¤ºè¯æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        prompts_file = self.prompts_dir / "all_prompts.json"
        if prompts_file.exists():
            self.log(f"æç¤ºè¯æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ç”Ÿæˆæ­¥éª¤: {prompts_file}")
            return True
        
        try:
            # åŠ è½½å¤„ç†åçš„æ•°æ®
            processed_data_file = self.processed_data_dir / "processed_samples.json"
            with open(processed_data_file, 'r', encoding='utf-8') as f:
                processed_samples = json.load(f)
            
            # åŠ è½½å®éªŒçŸ©é˜µ
            matrix_file = self.output_dir / "experimental_matrix.csv"
            matrix_df = pd.read_csv(matrix_file)
            
            # ç”Ÿæˆæ‰€æœ‰æç¤ºè¯
            self.log("æ­£åœ¨ç”Ÿæˆæç¤ºè¯...")
            all_prompts = self.prompt_generator.generate_all_prompts(
                matrix_file=str(matrix_file),
                data_dir=str(self.processed_data_dir)
            )
            
            # ä¿å­˜æç¤ºè¯
            prompts_file = self.prompts_dir / "all_prompts.json"
            text_file, json_file = self.prompt_generator.save_prompts(
                all_prompts, 
                output_file=str(self.prompts_dir / "prompts.txt"),
                json_output=str(prompts_file)
            )
            
            self.log(f"æˆåŠŸç”Ÿæˆ {len(all_prompts)} ä¸ªæç¤ºè¯")
            self.log(f"æç¤ºè¯å·²ä¿å­˜åˆ°: {prompts_file}")
            
            # ç”Ÿæˆæç¤ºè¯æ‘˜è¦
            self.prompt_generator.print_generation_summary(all_prompts)
            
            return True
            
        except Exception as e:
            self.log(f"ç”Ÿæˆæç¤ºè¯å¤±è´¥: {e}", "ERROR")
            return False
    
    def step4_setup_evaluation(self) -> bool:
        """æ­¥éª¤4: è®¾ç½®è¯„ä¼°æ¡†æ¶"""
        self.log("=" * 50)
        self.log("æ­¥éª¤4: è®¾ç½®è¯„ä¼°æ¡†æ¶")
        self.log("=" * 50)
        
        try:
            # ç”Ÿæˆè¯„ä¼°æ¨¡æ¿
            matrix_file = self.output_dir / "experimental_matrix.csv"
            batch_file = self.evaluation_framework.generate_evaluation_batch(
                str(matrix_file),
                str(self.evaluations_dir)
            )
            
            self.log(f"è¯„ä¼°æ¨¡æ¿å·²ç”Ÿæˆ: {batch_file}")
            self.log(f"è¯„ä¼°æŒ‡å—å·²ç”Ÿæˆ: {self.evaluations_dir / 'evaluation_guide.md'}")
            
            return True
            
        except Exception as e:
            self.log(f"è®¾ç½®è¯„ä¼°æ¡†æ¶å¤±è´¥: {e}", "ERROR")
            return False
    
    def step5_ai_image_generation(self) -> bool:
        """æ­¥éª¤5: AIå›¾åƒç”Ÿæˆ"""
        self.log("=" * 50)
        self.log("æ­¥éª¤5: AIå›¾åƒç”Ÿæˆ")
        self.log("=" * 50)
        
        try:
            # åˆ›å»ºAIå›¾åƒç”Ÿæˆå™¨
            generator = create_generator_from_config("ai_image_generator_config.json")
            
            # æ£€æŸ¥APIå¯†é’¥é…ç½®
            if generator.config.api_key == "your-api-key-here":
                self.log("è·³è¿‡AIå›¾åƒç”Ÿæˆ - æœªé…ç½®APIå¯†é’¥", "WARNING")
                return True
            
            # åŠ è½½æç¤ºè¯
            prompts_file = self.prompts_dir / "all_prompts.json"
            with open(prompts_file, 'r', encoding='utf-8') as f:
                all_prompts = json.load(f)
            
            # å‡†å¤‡ç”Ÿæˆæ•°æ®
            generation_data = []
            for prompt_data in all_prompts:
                generation_data.append({
                    'experiment_id': prompt_data['run_id'],
                    'prompt': prompt_data['prompt'],
                    'chart_type': prompt_data.get('chart_type', ''),
                    'layout_template': prompt_data.get('layout_template_id', '')
                })
            
            # æ‰¹é‡ç”Ÿæˆå›¾åƒ
            self.log(f"å¼€å§‹ç”Ÿæˆ {len(generation_data)} å¼ å›¾åƒ...")
            results = generator.batch_generate_images(generation_data)
            
            self.log(f"AIå›¾åƒç”Ÿæˆå®Œæˆ: {results.get('successful_generations', 0)}/{results.get('total_prompts', 0)}")
            
            # ä¿å­˜ç”Ÿæˆç»“æœ
            results_file = self.output_dir / "generation_results.json"
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            return True
            
        except Exception as e:
            self.log(f"AIå›¾åƒç”Ÿæˆå¤±è´¥: {e}", "ERROR")
            return False
    
    def step6_ai_evaluation(self) -> bool:
        """æ­¥éª¤6: AIè‡ªåŠ¨è¯„ä¼°"""
        self.log("=" * 50)
        self.log("æ­¥éª¤6: AIè‡ªåŠ¨è¯„ä¼°")
        self.log("=" * 50)
        
        try:
            # åˆ›å»ºAIè¯„ä¼°å™¨
            evaluator = create_evaluator_from_config()
            
            # æ£€æŸ¥APIå¯†é’¥é…ç½®
            if evaluator.config.api_key == "your-api-key-here":
                self.log("è·³è¿‡AIè‡ªåŠ¨è¯„ä¼° - æœªé…ç½®APIå¯†é’¥", "WARNING")
                return True
            
            # åŠ è½½å®éªŒçŸ©é˜µ
            matrix_file = self.output_dir / "experimental_matrix.csv"
            matrix_df = pd.read_csv(matrix_file)
            
            # å‡†å¤‡è¯„ä¼°æ•°æ®
            eval_data = []
            for _, exp in matrix_df.iterrows():
                eval_data.append({
                    'experiment_id': exp['Run ID'],
                    'data_info': f"æ•°æ®æ ·æœ¬ID: {exp['Data Sample ID']}",
                    'chart_type': exp['Chart Type'],
                    'layout_template': exp['Layout Template ID']
                })
            
            # æ‰¹é‡è¯„ä¼°å›¾åƒ
            images_dir = "generated_images"
            self.log(f"å¼€å§‹è¯„ä¼° {len(eval_data)} å¼ å›¾åƒ...")
            results = evaluator.batch_evaluate_images(images_dir, eval_data)
            
            self.log(f"AIè‡ªåŠ¨è¯„ä¼°å®Œæˆ: {results.get('successful_evaluations', 0)}/{results.get('total_images', 0)}")
            
            # ä¿å­˜è¯„ä¼°ç»“æœ
            results_file = self.evaluations_dir / "ai_evaluation_results.json"
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            return True
            
        except Exception as e:
            self.log(f"AIè‡ªåŠ¨è¯„ä¼°å¤±è´¥: {e}", "ERROR")
            return False
    
    def generate_experiment_report(self) -> str:
        """ç”Ÿæˆç»¼åˆå®éªŒæŠ¥å‘Š"""
        self.log("=" * 50)
        self.log("ç”Ÿæˆç»¼åˆå®éªŒæŠ¥å‘Š")
        self.log("=" * 50)
        
        end_time = datetime.now()
        duration = end_time - self.start_time if self.start_time else "æœªçŸ¥"
        
        # åŠ è½½ç”Ÿæˆå’Œè¯„ä¼°ç»“æœ
        generation_results = {}
        evaluation_results = {}
        
        try:
            gen_file = self.output_dir / "generation_results.json"
            if gen_file.exists():
                with open(gen_file, 'r', encoding='utf-8') as f:
                    generation_results = json.load(f)
        except:
            pass
        
        try:
            eval_file = self.evaluations_dir / "ai_evaluation_results.json"
            if eval_file.exists():
                with open(eval_file, 'r', encoding='utf-8') as f:
                    evaluation_results = json.load(f)
        except:
            pass
        
        # Format time strings outside f-string to avoid backslash issue
        start_time_str = self.start_time.strftime('%Y-%m-%d %H:%M:%S') if self.start_time else 'æœªçŸ¥'
        end_time_str = end_time.strftime('%Y-%m-%d %H:%M:%S')
        
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_content = self._generate_report_template(
            start_time_str, end_time_str, duration, 
            generation_results, evaluation_results
        )
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.reports_dir / f"{self.experiment_id}_comprehensive_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        self.log(f"ç»¼åˆå®éªŒæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return str(report_file)
    
    def _generate_performance_metrics(self, evaluation_results: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ€§èƒ½æŒ‡æ ‡æ–‡æœ¬"""
        if evaluation_results.get('status') == 'skipped' or not evaluation_results.get('evaluation_results'):
            return "- **çŠ¶æ€**: æœªæ‰§è¡Œè¯„ä¼°æˆ–æ— è¯„ä¼°æ•°æ®"
        
        eval_data = evaluation_results.get('evaluation_results', [])
        if not eval_data:
            return "- **çŠ¶æ€**: æ— è¯„ä¼°æ•°æ®"
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        total_scores = [result.get('overall_score', 0) for result in eval_data]
        data_scores = [result.get('data_consistency', {}).get('total_score', 0) for result in eval_data]
        layout_scores = [result.get('layout_accuracy', {}).get('total_score', 0) for result in eval_data]
        aesthetic_scores = [result.get('aesthetic_quality', {}).get('total_score', 0) for result in eval_data]
        
        avg_total = sum(total_scores) / len(total_scores) if total_scores else 0
        avg_data = sum(data_scores) / len(data_scores) if data_scores else 0
        avg_layout = sum(layout_scores) / len(layout_scores) if layout_scores else 0
        avg_aesthetic = sum(aesthetic_scores) / len(aesthetic_scores) if aesthetic_scores else 0
        
        return f"""
            - **å¹³å‡æ€»åˆ†**: {avg_total:.2f}/30 ({avg_total/30*100:.1f}%)
            - **å¹³å‡æ•°æ®ä¸€è‡´æ€§**: {avg_data:.2f}/10
            - **å¹³å‡å¸ƒå±€å‡†ç¡®æ€§**: {avg_layout:.2f}/10
            - **å¹³å‡ç¾è§‚åº¦**: {avg_aesthetic:.2f}/10
            - **æœ€é«˜åˆ†**: {max(total_scores) if total_scores else 0:.2f}/30
            - **æœ€ä½åˆ†**: {min(total_scores) if total_scores else 0:.2f}/30
        """
    
    def _generate_next_steps(self, generation_results: Dict[str, Any], evaluation_results: Dict[str, Any]) -> str:
        """ç”Ÿæˆä¸‹ä¸€æ­¥å»ºè®®"""
        steps = []
        
        if generation_results.get('status') == 'skipped':
            steps.append("1. é…ç½®AIå›¾åƒç”ŸæˆAPIå¯†é’¥ä»¥å¯ç”¨è‡ªåŠ¨å›¾åƒç”Ÿæˆ")
        elif generation_results.get('failed_generations', 0) > 0:
            steps.append("1. æ£€æŸ¥å¹¶ä¼˜åŒ–å¤±è´¥çš„å›¾åƒç”Ÿæˆä»»åŠ¡")
        
        if evaluation_results.get('status') == 'skipped':
            steps.append("2. é…ç½®AIè¯„ä¼°APIå¯†é’¥ä»¥å¯ç”¨è‡ªåŠ¨è´¨é‡è¯„ä¼°")
        elif evaluation_results.get('failed_evaluations', 0) > 0:
            steps.append("2. æ£€æŸ¥å¹¶é‡æ–°è¯„ä¼°å¤±è´¥çš„å›¾åƒ")
        
        if generation_results.get('status') != 'skipped' and evaluation_results.get('status') != 'skipped':
            steps.extend([
                "3. åˆ†æè¯„ä¼°ç»“æœï¼Œè¯†åˆ«æœ€ä½³å®è·µæ¨¡å¼",
                "4. åŸºäºè¯„ä¼°åé¦ˆä¼˜åŒ–æç¤ºè¯æ¨¡æ¿",
                "5. è¿›è¡ŒA/Bæµ‹è¯•éªŒè¯æ”¹è¿›æ•ˆæœ",
                "6. æ‰©å±•å®éªŒåˆ°æ›´å¤šæ•°æ®æ ·æœ¬å’Œå›¾è¡¨ç±»å‹"
            ])
        else:
            steps.extend([
                "3. å®ŒæˆAPIé…ç½®åé‡æ–°è¿è¡Œå®Œæ•´å®éªŒ",
                "4. åˆ†æç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–æµç¨‹çš„æ€§èƒ½è¡¨ç°"
            ])
        
        return "\n".join(steps)
    
    def run_full_experiment(self, experiment_name: str = None) -> bool:
        """è¿è¡Œå®Œæ•´çš„ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–å®éªŒæµç¨‹"""
        try:
            # å¼€å§‹å®éªŒ
            exp_id = self.start_experiment(experiment_name)
            
            # åˆå§‹åŒ–ç»„ä»¶
            if not self.initialize_components():
                self.log("ç»„ä»¶åˆå§‹åŒ–å¤±è´¥ï¼Œå®éªŒç»ˆæ­¢", "ERROR")
                return False
            
            # æ‰§è¡Œå®éªŒæ­¥éª¤
            steps = [
                ("æ•°æ®é¢„å¤„ç†", self.step1_process_data),
                ("ç”Ÿæˆå®éªŒçŸ©é˜µ", self.step2_generate_matrix),
                ("ç”Ÿæˆæç¤ºè¯", self.step3_generate_prompts),
                ("è®¾ç½®è¯„ä¼°æ¡†æ¶", self.step4_setup_evaluation),
                ("AIå›¾åƒç”Ÿæˆ", self.step5_ai_image_generation),
                ("AIè‡ªåŠ¨è¯„ä¼°", self.step6_ai_evaluation)
            ]
            
            for step_name, step_func in steps:
                self.log(f"å¼€å§‹æ‰§è¡Œ: {step_name}")
                if not step_func():
                    self.log(f"{step_name} æ‰§è¡Œå¤±è´¥ï¼Œå®éªŒç»ˆæ­¢", "ERROR")
                    return False
                self.log(f"{step_name} æ‰§è¡Œå®Œæˆ")
            
            # ç”Ÿæˆç»¼åˆå®éªŒæŠ¥å‘Š
            report_file = self.generate_experiment_report()
            
            self.log("=" * 60)
            self.log("ğŸ‰ ç«¯åˆ°ç«¯AIè‡ªåŠ¨åŒ–å®éªŒæ‰§è¡Œå®Œæˆï¼")
            self.log("=" * 60)
            self.log(f"å®éªŒID: {exp_id}")
            self.log(f"ç»¼åˆæŠ¥å‘Š: {report_file}")
            self.log(f"è¾“å‡ºç›®å½•: {self.output_dir}")
            
            return True
            
        except Exception as e:
            self.log(f"å®éªŒæ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", "ERROR")
            return False
    
    def analyze_results(self, evaluations_dir: str = None) -> Optional[str]:
        """åˆ†æè¯„ä¼°ç»“æœ"""
        if not evaluations_dir:
            evaluations_dir = str(self.evaluations_dir)
        
        try:
            self.log("å¼€å§‹åˆ†æè¯„ä¼°ç»“æœ...")
            
            # åˆ†æè¯„ä¼°ç»“æœ
            results_df = self.evaluation_framework.analyze_evaluation_results(evaluations_dir)
            
            if results_df.empty:
                self.log("æœªæ‰¾åˆ°è¯„ä¼°ç»“æœæ–‡ä»¶", "WARNING")
                return None
            
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            report_file = self.reports_dir / "evaluation_analysis_report.md"
            self.evaluation_framework.generate_evaluation_report(
                results_df, 
                str(report_file)
            )
            
            self.log(f"è¯„ä¼°åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
            return str(report_file)
            
        except Exception as e:
            self.log(f"åˆ†æè¯„ä¼°ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}", "ERROR")
            return None
    
    def _load_config(self, config_path: str = None) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            'llm_config': {},
            'random_seed': 42,
            'data_quality_threshold': 0.5,
            'enable_intelligent_processing': True,
            'fallback_mode': True
        }
        
        if config_path is None:
            # å°è¯•åŠ è½½é»˜è®¤é…ç½®æ–‡ä»¶
            config_files = ['llm_config.json', 'config.json']
            for config_file in config_files:
                config_path = Path(self.benchmark_data_dir).parent / config_file
                if config_path.exists():
                    break
            else:
                print("æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return default_config
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                # åˆå¹¶é»˜è®¤é…ç½®
                for key, value in default_config.items():
                    if key not in config:
                        config[key] = value
                return config
        except Exception as e:
            print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return default_config

    def _generate_report_template(self, start_time_str: str, end_time_str: str, 
                                duration, generation_results: Dict[str, Any], 
                                evaluation_results: Dict[str, Any]) -> str:
        return f"""
            # ChartGalaxyç«¯åˆ°ç«¯AIè‡ªåŠ¨åŒ–ä¿¡æ¯å›¾ç”Ÿæˆå®éªŒæŠ¥å‘Š

            ## ğŸ¯ å®éªŒåŸºæœ¬ä¿¡æ¯

            - **å®éªŒID**: {self.experiment_id}
            - **å®éªŒç±»å‹**: ç«¯åˆ°ç«¯AIè‡ªåŠ¨åŒ–æ•°æ®é©±åŠ¨ä¿¡æ¯å›¾ç”Ÿæˆ
            - **å¼€å§‹æ—¶é—´**: {start_time_str}
            - **ç»“æŸæ—¶é—´**: {end_time_str}
            - **å®éªŒè€—æ—¶**: {str(duration)}
            - **å®éªŒç›®æ ‡**: éªŒè¯ChartGalaxyæ–¹æ³•è®ºçš„ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–èƒ½åŠ›

            ## ğŸ”„ è‡ªåŠ¨åŒ–æµç¨‹çŠ¶æ€

            - **æ•°æ®é¢„å¤„ç†**: âœ… è‡ªåŠ¨åŒ–å®Œæˆ
            - **å®éªŒè®¾è®¡**: âœ… è‡ªåŠ¨åŒ–å®Œæˆ
            - **æç¤ºè¯ç”Ÿæˆ**: âœ… è‡ªåŠ¨åŒ–å®Œæˆ
            - **å›¾åƒç”Ÿæˆ**: {'âœ… AIè‡ªåŠ¨åŒ–' if generation_results.get('status') != 'skipped' else 'â­ï¸ å·²è·³è¿‡'}
            - **è´¨é‡è¯„ä¼°**: {'âœ… AIè‡ªåŠ¨åŒ–' if evaluation_results.get('status') != 'skipped' else 'â­ï¸ å·²è·³è¿‡'}

            ## ğŸ“Š å®éªŒè®¾è®¡

            ### æ•°æ®æ¥æº
            - **æ•°æ®é›†**: MatPlotBench
            - **æ•°æ®ç›®å½•**: {self.benchmark_data_dir}
            - **é€‰æ‹©æ ·æœ¬**: å‰3ä¸ªæ•°æ®æ ·æœ¬

            ## ğŸ¨ AIå›¾åƒç”Ÿæˆç»“æœ

            - **çŠ¶æ€**: {generation_results.get('status', 'æœªæ‰§è¡Œ')}
            - **æ€»å°è¯•æ•°**: {generation_results.get('total_prompts', 0)}
            - **æˆåŠŸç”Ÿæˆ**: {generation_results.get('successful_generations', 0)}
            - **å¤±è´¥ç”Ÿæˆ**: {generation_results.get('failed_generations', 0)}
            - **æˆåŠŸç‡**: {generation_results.get('successful_generations', 0) / max(generation_results.get('total_prompts', 1), 1) * 100:.1f}%

            ## ğŸ” AIè‡ªåŠ¨è¯„ä¼°ç»“æœ

            - **çŠ¶æ€**: {evaluation_results.get('status', 'æœªæ‰§è¡Œ')}
            - **æ€»è¯„ä¼°æ•°**: {evaluation_results.get('total_images', 0)}
            - **æˆåŠŸè¯„ä¼°**: {evaluation_results.get('successful_evaluations', 0)}
            - **å¤±è´¥è¯„ä¼°**: {evaluation_results.get('failed_evaluations', 0)}
            - **æˆåŠŸç‡**: {evaluation_results.get('successful_evaluations', 0) / max(evaluation_results.get('total_images', 1), 1) * 100:.1f}%

            ## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡

            {self._generate_performance_metrics(evaluation_results)}

            ## å®éªŒè¾“å‡ºæ–‡ä»¶

            ### æ•°æ®å¤„ç†
            - `{self.processed_data_dir / 'processed_samples.json'}`: é¢„å¤„ç†åçš„æ•°æ®æ ·æœ¬

            ### å®éªŒè®¾è®¡
            - `{self.output_dir / 'experimental_matrix.csv'}`: å®Œæ•´å®éªŒçŸ©é˜µ

            ### æç¤ºè¯ç”Ÿæˆ
            - `{self.prompts_dir / 'all_prompts.json'}`: æ‰€æœ‰å®éªŒæ¡ä»¶çš„æç¤ºè¯
            - `{self.prompts_dir / 'generation_summary.txt'}`: æç¤ºè¯ç”Ÿæˆæ‘˜è¦

            ### AIç”Ÿæˆç»“æœ
            - `generated_images/`: AIç”Ÿæˆçš„ä¿¡æ¯å›¾å›¾åƒ
            - `{self.output_dir / 'generation_results.json'}`: å›¾åƒç”Ÿæˆç»“æœç»Ÿè®¡

            ### AIè¯„ä¼°ç»“æœ
            - `{self.evaluations_dir / 'ai_evaluation_results.json'}`: AIè‡ªåŠ¨è¯„ä¼°ç»“æœ
            - `{self.evaluations_dir / 'evaluation_guide.md'}`: è¯„ä¼°æŒ‡å—æ–‡æ¡£

            ## ğŸš€ ä¸‹ä¸€æ­¥æ“ä½œ

            {self._generate_next_steps(generation_results, evaluation_results)}

            ## å®éªŒæ—¥å¿—

            {chr(96)*3}
            {"###########next#############".join(self.experiment_log)}
            {chr(96)*3}

            ---
            *æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {end_time_str}*  """

def main():
    """ä¸»å‡½æ•°"""
    print("ChartGalaxy ç«¯åˆ°ç«¯AIè‡ªåŠ¨åŒ–ä¿¡æ¯å›¾ç”Ÿæˆå®éªŒ")
    print("=" * 60)
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    benchmark_dir = "benchmark_data"
    if not Path(benchmark_dir).exists():
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®ç›®å½• '{benchmark_dir}'")
        print("è¯·ç¡®ä¿MatPlotBenchæ•°æ®å·²æ­£ç¡®æ”¾ç½®åœ¨benchmark_dataç›®å½•ä¸­")
        return
    
    # åˆ›å»ºå®éªŒè¿è¡Œå™¨ï¼ˆåŠ è½½é…ç½®ï¼‰
    runner = ExperimentRunner(
        benchmark_data_dir=benchmark_dir,
        output_dir="experiment_output",
        config_path="llm_config.json"
    )
    
    # è¿è¡Œå®Œæ•´çš„ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–å®éªŒ
    success = runner.run_full_experiment("chartgalaxy_e2e_automation_exp")
    
    if success:
        print("\n" + "=" * 60)
        print("ğŸ‰ ç«¯åˆ°ç«¯AIè‡ªåŠ¨åŒ–å®éªŒæ‰§è¡ŒæˆåŠŸå®Œæˆï¼")
        print("=" * 60)
        print("\nâœ… å·²å®Œæˆçš„è‡ªåŠ¨åŒ–æµç¨‹:")
        print("1. âœ… æ•°æ®é¢„å¤„ç†")
        print("2. âœ… å®éªŒçŸ©é˜µç”Ÿæˆ")
        print("3. âœ… æç¤ºè¯ç”Ÿæˆ")
        print("4. ğŸ¨ AIå›¾åƒç”Ÿæˆ (å¦‚å·²é…ç½®API)")
        print("5. ğŸ” AIè‡ªåŠ¨è¯„ä¼° (å¦‚å·²é…ç½®API)")
        print("6. âœ… ç»¼åˆæŠ¥å‘Šç”Ÿæˆ")
        print(f"\nğŸ“ æ‰€æœ‰è¾“å‡ºæ–‡ä»¶ä½äº: {runner.output_dir}")
        print(f"ğŸ“‹ æŸ¥çœ‹ç»¼åˆæŠ¥å‘Šäº†è§£è¯¦ç»†ç»“æœ")
    else:
        print("\n" + "=" * 60)
        print("âŒ å®éªŒæ‰§è¡Œå¤±è´¥ï¼")
        print("=" * 60)
        print("è¯·æŸ¥çœ‹æ—¥å¿—ä¿¡æ¯æ’æŸ¥é—®é¢˜")

if __name__ == "__main__":
    main()